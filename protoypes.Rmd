---
title: "Heart Disease Prediction"
author: "Alex Kim"
subtitle: 'Stats 202: Data Mining and Analysis'
---

```{R message=FALSE}
library(tidyverse)
library(glmnet)  # logistic regression (glm)
library(MASS)  # LDA, QDA
library(class)  # KNN
```

***

# Importing the Data

```{R message=FALSE}
# Initial import
train_data <- read_csv("data/train_data.csv")
test_data <- read_csv("data/test_data.csv")

# Remove ID's; create ID vector for test
train_data <- train_data[-12]
test_ids <- as.vector(test_data$Id)
test_data <- test_data[-11]
```

 * **train_data** contains all predictors and the outcome variable "Status."
 * **test_data** contains only the predictors.

***

# Summary Statistics

### Dimensions

```{R}
dim(train_data)
```

### Variance

Variance for every column except ID

```{R}
apply(train_data, 2, var)
```

### Outcome Totals

```{R}
sum(train_data$Status)
nrow(train_data) - sum(train_data$Status)
```

### PCA

```{R}
obs_pc <- prcomp(train_data[,-11])$x[,c(1,2)]
plot(obs_pc, col = train_data$Status + 1)
```

***

# Data Preparation

### Subtraining and Subtest Datasets

The file *test_data.csv* does not contain response variables; therefore, we cannot use it to evaluate the error of our models. We instead divide the training dataset into a "subtraining" and "subtest" set to allow us to evaluate the error of our models. We follow an 80:20 subtraining:subtest split.

```{R}
set.seed(1)

subtrain_size <- floor(0.8 * nrow(train_data))
subtrain_indexes <- sample(x = 1:nrow(train_data), size = subtrain_size)

subtrain <- train_data[subtrain_indexes,]
subtest <- train_data[-subtrain_indexes,]

rm(subtrain_size, subtrain_indexes)
```

***

# Logistic Regression

### Standard

#### Pre-Evaluation

Error: **0.6**

```{R}
# Train a logistic regression model
lreg <- glm(formula = Status ~ ., data = subtrain, family = binomial)

# Predict the response on the test dataset
prob_predictions <- predict(lreg, subtest[,-11], type = "response")
binary_predictions <- if_else(prob_predictions >= 0.5, TRUE, FALSE)

# Evaluate error of subtest predictions
true_responses <- subtest$Status
error <- mean(binary_predictions == true_responses)
error

# Clean up extraneous variables
rm(lreg, true_responses, binary_predictions, prob_predictions)
```

#### Kaggle

Kaggle Score: **0.65000**

```{R}
# Train a logistic regression model
lreg <- glm(formula = Status ~ ., data = train_data, family = binomial)

# Predict the response on the test dataset
prob_predictions <- predict(lreg, test_data, type = "response")
binary_predictions <- if_else(prob_predictions >= 0.5, TRUE, FALSE)

# Create an output CSV containing ID and predicted status
output <- tibble(Id = test_ids, Category = binary_predictions)
write_csv(output, "data/submission.csv")
```

### Ridge Regularization

#### Pre-Evaluation

Error: **0.61**

```{R}
# Train a logistic regression model
predictors <- as.matrix(subtrain[,-11])
response <- as.matrix(subtrain[,11])

ridge_lreg <- cv.glmnet(predictors, response, alpha = 0)
optimal_lambda <- ridge_lreg$lambda.min

optimal_ridge_lreg <- glmnet(predictors, response, alpha = 0, lambda = optimal_lambda)

# Predict the response on the test dataset
prob_predictions <- predict(optimal_ridge_lreg, s = optimal_lambda, newx = as.matrix(subtest[,-11]))
binary_predictions <- if_else(prob_predictions >= 0.5, TRUE, FALSE)

# Evaluate error of subtest predictions
true_responses <- subtest$Status
error <- mean(binary_predictions == true_responses)
error
```

#### Kaggle

Kaggle Score: **0.66666**

```{R}
# Train a logistic regression model
predictors <- as.matrix(train_data[,-11])
response <- as.matrix(train_data[,11])

ridge_lreg <- cv.glmnet(predictors, response, alpha = 0)
optimal_lambda <- ridge_lreg$lambda.min

optimal_ridge_lreg <- glmnet(predictors, response, alpha = 0, lambda = optimal_lambda)

# Predict the response on the test dataset
prob_predictions <- predict(optimal_ridge_lreg, s = optimal_lambda, newx = as.matrix(test_data))
binary_predictions <- if_else(prob_predictions >= 0.5, TRUE, FALSE)

# Create an output CSV containing ID and predicted status
output <- tibble(Id = test_ids, Category = binary_predictions)
write_csv(output, "data/submission.csv")
```

***

# Linear Discriminant Analysis

### Pre-Evaluation

Error: 0.6

```{R}
# Train a LDA model
lda_model <- lda(Status ~ ., data = subtrain)

# Predict the response on the test dataset
predictions <- predict(lda_model, subtest)$class

# Evaluate the error
true_responses <- as.list(subtest$Status)
error <- mean(predictions == true_responses)
error
```

# Quadratic Discriminant Analysis

### Pre-Evaluation 1: Baseline

Mean Accuracy: 0.67

```{R}
# Train a QDA model
qda_model <- qda(Status ~ ., data = subtrain)

# Predict the response on the test dataset
predictions <- predict(qda_model, subtest)$class

# Evaluate the error
true_responses <- as.list(subtest$Status)
error <- mean(predictions == true_responses)
error
```

### Pre-Evaluation 2: 

 * Center data and standardize variance to 1 (no significant difference in result)
 * QDA, try different probability thresholds
 * Try different combinations of predictors
 * **Don't use this predictor for Kaggle submission** unless Kaggle submission is scaled first

```{R}
# See p-values of logistic regression predictors
lreg <- glm(formula = Status ~ ., data = subtrain, family = binomial)
summary(lreg)
```

```{R}
# Scale all predictors in subtest and subtrain
subtrain_status <- subtrain$Status
subtest_status <- subtest$Status

subtrain <- as_tibble(scale(subtrain))
subtest <- as_tibble(scale(subtest))

subtrain <- mutate(subtrain, Status = subtrain_status)
subtest <- mutate(subtest, Status = subtest_status)

# Train a QDA model
#qda_model <- qda(Status ~ ., data = subtrain)
qda_model <- qda(Status ~ age + assay, data = subtrain)

# Predict the response on the test dataset
predictions <- predict(qda_model, subtest)$class

# Evaluate the error
true_responses <- as.list(subtest$Status)
error <- mean(predictions == true_responses)
error
```

### Kaggle

```{R}
# Train a QDA model
qda_model <- qda(Status ~ ., data = train_data)

# Predict the response on the test dataset
predictions <- predict(qda_model, test_data)$class
predictions <- if_else(predictions == 1, TRUE, FALSE)

# Create an output CSV containing ID and predicted status
output <- tibble(Id = test_ids, Category = predictions)
write_csv(output, "data/submission.csv")
```

***

# K-Nearest Neighbors

### Pre-Evaluation

```{R}
set.seed(1)

# Train a KNN model
train_predictors <- as.matrix(subtrain[,-11])
train_response <- as.matrix(subtrain[,11])
test_predictors <- as.matrix(subtest[,-11])

predictions <- knn(train_predictors, test_predictors, train_response, k = 20)

true_responses <- as.list(subtest$Status)
mean(predictions == true_responses)
```

# Support Vector Machine

### Pre-Evaluation

```{R}
set.seed(1)
dat <- data.frame(x = as.matrix(subtrain[,-11]), y = as.factor(subtrain$Status))

library(e1071)
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)

tune_out <- tune(svm, y ~ ., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))

bestmod <- summary(tune_out)$best.model

dat_test <- data.frame(x = as.matrix(subtest[,-11]), y = as.factor(subtest$Status))
ypred <- predict(bestmod, dat_test)

accuracy <- mean(ypred == dat_test$y)
```